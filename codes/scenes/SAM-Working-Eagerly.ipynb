{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186aadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import tf_datasets\n",
    "# import tf_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e4b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(keras.Model):\n",
    "#     def train_step(self, data):\n",
    "#         inputs, targets = data\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         with tf.GradientTape() as tape2:\n",
    "#             with tf.GradientTape() as tape1:\n",
    "#                 preds = self(inputs, training=True)  # Forward pass\n",
    "#                 # Compute the loss value\n",
    "#                 # (the loss function is configured in `compile()`)\n",
    "#                 loss = self.compiled_loss(targets, preds)\n",
    "#             # Compute first-order gradients\n",
    "#             dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "#         # Compute second-order gradients\n",
    "#         d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "#         # Combine first-order and second-order gradients\n",
    "#         grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(targets, preds)\n",
    "#         # Return a dict mapping metric names to current value\n",
    "#         return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609b261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Construct an instance of MyModel\n",
    "# def get_model():\n",
    "#     inputs = keras.Input(shape=(784,))\n",
    "#     intermediate = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "#     outputs = layers.Dense(10, activation=\"softmax\")(intermediate)\n",
    "#     model = MyModel(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Prepare data\n",
    "# (x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "# x_train = np.reshape(x_train, (-1, 784)) / 255\n",
    "\n",
    "# model = get_model()\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "# model.fit(x_train, y_train, epochs=3, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288b8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlayers = [27, 34, 2]\n",
    "mlayers = [27, 34, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14646c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model():\n",
    "    inputs = keras.layers.Input(27 ,)\n",
    "    \n",
    "    y1 = keras.layers.Input(2 ,)\n",
    "    y2 = keras.layers.Input(2 ,)\n",
    "    \n",
    "    x = inputs\n",
    "    for i in range(1, len(dlayers)-1):\n",
    "        x = keras.layers.Dense(dlayers[i], activation=\"tanh\", \n",
    "                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                              bias_initializer=tf.keras.initializers.Constant(0.5))(x)    \n",
    "        if 1:\n",
    "            x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "        x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(dlayers[-1], activation=\"tanh\", \n",
    "                          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                          bias_initializer=tf.keras.initializers.Constant(0.5))(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "    output1 = x\n",
    "    \n",
    "    x = inputs\n",
    "    for i in range(1, len(mlayers)-1):\n",
    "        x = keras.layers.Dense(mlayers[i], activation=\"tanh\", \n",
    "                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                              bias_initializer=tf.keras.initializers.Constant(0.5))(x)    \n",
    "        if 1:\n",
    "            x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "        x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(mlayers[-1], activation=\"tanh\", \n",
    "                          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                          bias_initializer=tf.keras.initializers.Constant(0.5))(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "    output2 = x\n",
    "    \n",
    "    x = inputs\n",
    "    for i in range(1, len(mlayers)-1):\n",
    "        x = keras.layers.Dense(mlayers[i], activation=\"tanh\", \n",
    "                          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                          bias_initializer=tf.keras.initializers.Constant(0.5))(x)   \n",
    "        if 1:\n",
    "            x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "        x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(mlayers[-1], activation=\"tanh\", \n",
    "                          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1),\n",
    "                          bias_initializer=tf.keras.initializers.Constant(0.5))(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.999, #0.999 \n",
    "                epsilon=0.0001, # 1e-4\n",
    "             )(x)\n",
    "    output3 = x\n",
    "        \n",
    "    model = keras.Model([inputs, y1, y2], [output1, output2, output3])\n",
    "    return model, y1, y2, output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fa16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig, y1, y2, out1, out2, out3 = training_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740e958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "class SAMModel(tf.keras.Model):\n",
    "    def __init__(self, model, rho=0.05):\n",
    "        \"\"\"\n",
    "        p, q = 2 for optimal results as suggested in the paper\n",
    "        (Section 2)\n",
    "        \"\"\"\n",
    "        super(SAMModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.rho = rho\n",
    "        \n",
    "#     def call(self, data):\n",
    "#         return self.model(data)\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             predictions = self.model(data)\n",
    "#             loss = self.compiled_loss((data, predictions), predictions)\n",
    "#         return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        inputs = data\n",
    "        (inp, y1, y2) = data[0]\n",
    "        labels = y1\n",
    "        e_ws = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(data)\n",
    "            [out1, out2, out3] = predictions \n",
    "            \n",
    "            loss_detector = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y1, logits=out1))\n",
    "            loss_modifier = tf.nn.l2_loss(out2 - y2)\n",
    "\n",
    "            loss_modifier_mve = 0.5*tf.reduce_sum(((out2 - y2)**2)/(out3**2 + 1e-4)) + 0.5*tf.reduce_sum(tf.math.log(out3**2 + 1e-4)) # mean variance estimate\n",
    "\n",
    "            loss = loss_detector + loss_modifier\n",
    "            loss_mve = loss_detector + loss_modifier_mve\n",
    "            loss = loss + loss_mve\n",
    "        \n",
    "        trainable_params = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_params)\n",
    "        grad_norm = self._grad_norm(gradients)\n",
    "        scale = self.rho / (grad_norm + 1e-12)\n",
    "\n",
    "        for (grad, param) in zip(gradients, trainable_params):\n",
    "            e_w = grad * scale\n",
    "            param.assign_add(e_w)\n",
    "            e_ws.append(e_w)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(data)\n",
    "            [out1, out2, out3] = predictions \n",
    "            \n",
    "            loss_detector = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y1, logits=out1))\n",
    "            loss_modifier = tf.nn.l2_loss(out2 - y2)\n",
    "\n",
    "            loss_modifier_mve = 0.5*tf.reduce_sum(((out2 - y2)**2)/(out3**2 + 1e-4)) + 0.5*tf.reduce_sum(tf.math.log(out3**2 + 1e-4)) # mean variance estimate\n",
    "\n",
    "            loss = loss_detector + loss_modifier\n",
    "            loss_mve = loss_detector + loss_modifier_mve\n",
    "            loss = loss + loss_mve\n",
    "        \n",
    "        sam_gradients = tape.gradient(loss, trainable_params)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws):\n",
    "            param.assign_sub(e_w)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(sam_gradients, trainable_params))\n",
    "        \n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (inp, y1, y2) = data[0]\n",
    "        predictions = self.model(data, training=False)\n",
    "        [out1, out2, out3] = predictions \n",
    "    \n",
    "        loss_detector = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y1, logits=out1))\n",
    "        loss_modifier = tf.nn.l2_loss(out2 - y2)\n",
    "\n",
    "        loss_modifier_mve = 0.5*tf.reduce_sum(((out2 - y2)**2)/(out3**2 + 1e-4)) + 0.5*tf.reduce_sum(tf.math.log(out3**2 + 1e-4)) # mean variance estimate\n",
    "\n",
    "        loss = loss_detector + loss_modifier\n",
    "        loss_mve = loss_detector + loss_modifier_mve\n",
    "\n",
    "        loss = loss + loss_mve\n",
    "        \n",
    "#         loss = self.compiled_loss(labels, predictions)\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def _grad_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([\n",
    "                tf.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae4cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAMModel(model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9843f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_detector = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y1, logits=out1))\n",
    "loss_modifier = tf.nn.l2_loss(out2 - y2)\n",
    "\n",
    "loss_modifier_mve = 0.5*tf.reduce_sum(((out2 - y2)**2)/(out3**2 + 1e-4)) + 0.5*tf.reduce_sum(tf.math.log(out3**2 + 1e-4)) # mean variance estimate\n",
    "\n",
    "loss = loss_detector + loss_modifier\n",
    "model.add_loss(loss)\n",
    "\n",
    "loss_mve = loss_detector + loss_modifier_mve\n",
    "model.add_loss(loss_mve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea17115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=opt\n",
    "             ,run_eagerly=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a67981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_datasets\n",
    "\n",
    "data_sets, N_tuple = tf_datasets.read_data_sets(dirs=sorted([\"../training_data/Original/tdata/\"]), use_softmax=True, frac_test=0.25)\n",
    "scale = { i: max(abs(data_sets.train.get_data()[i].min()), abs(data_sets.train.get_data()[i].max())) for i in data_sets.train.get_data() }\n",
    "\n",
    "train_data = data_sets.train.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086c38d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6282, 27)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"inputs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e9fd4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 22ms/step\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_data[\"inputs\"], train_data[\"labels\"], train_data[\"modvel\"]/scale[\"modvel\"]], batch_size=5000, epochs=100, verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4e02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d36690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecc0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

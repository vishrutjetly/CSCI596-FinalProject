{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25ca8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys, os, argparse, shutil, inspect\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tf_datasets, tf_network\n",
    "\n",
    "import logging, pickle\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d655ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['scenes/tf_train.py', '--mve', '-o', '/tmp/tfmodel/', '/tmp/tdata/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb08972",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Generate Training Data', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('-o', '--output', default='/tmp/tfmodel/',   help='output directory')\n",
    "parser.add_argument('-l', '--load',   action=\"store_true\",       help='load and resume the training')\n",
    "parser.add_argument(      '--lstep',  default=100,   type=int,   help='log summary; e.g., every 10th steps')\n",
    "parser.add_argument('-s', '--steps',  default=1000, type=int,   help='maximum training steps')\n",
    "parser.add_argument('-b', '--batch',  default=5000,  type=int,   help='batch size for one step training')\n",
    "parser.add_argument('-t', '--ftest',  default=0.25,  type=float, help='fraction for the test data set')\n",
    "parser.add_argument('-d', '--dnet',   default='27-34-2',         help='detection networks int-int-...')\n",
    "parser.add_argument('-m', '--mnet',   default='27-34-2',         help='modification networks int-int-...')\n",
    "parser.add_argument(      '--dact',   default='none-tanh-tanh',  help='activation function for detection networks')\n",
    "parser.add_argument(      '--mact',   default='none-tanh-tanh',  help='activation function for modification networks')\n",
    "parser.add_argument(      '--stats',  action=\"store_true\",       help='write the stats')\n",
    "parser.add_argument('-v', '--mve',    action=\"store_true\",       help='turn on mean-variance learning')\n",
    "parser.add_argument(      '--nosmax', action=\"store_true\",       help='do not use the softmax model')\n",
    "parser.add_argument('-r', '--decay',  default=0.1,   type=float, help='regularization coefficient')\n",
    "parser.add_argument(      '--ddrop',  default=0.1,   type=float, help='dropout rate (detection)')\n",
    "parser.add_argument(      '--mdrop',  default=0.1,   type=float, help='dropout rate (modification)')\n",
    "parser.add_argument('datadirs', action=\"store\", nargs=\"+\",       help='path(s) to the training data')\n",
    "pargs = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f55620",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8caa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pargs.dnet = \"27-34-17-2\"\n",
    "# pargs.mnet = \"27-34-17-2\"\n",
    "# pargs.dact = \"none-tanh-tanh-tanh\"\n",
    "# pargs.mact = \"none-tanh-tanh-tanh\"\n",
    "\n",
    "pargs.output = \"models/tfmodel_normal/\"\n",
    "pargs.dnet = \"27-34-2\"\n",
    "pargs.mnet = \"27-34-2\"\n",
    "pargs.dact = \"none-tanh-tanh\"\n",
    "pargs.mact = \"none-tanh-tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4f4b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch=5000, dact='none-tanh-tanh', datadirs=['/tmp/tdata/'], ddrop=0.1, decay=0.1, dnet='27-34-2', ftest=0.25, load=False, lstep=100, mact='none-tanh-tanh', mdrop=0.1, mnet='27-34-2', mve=True, nosmax=False, output='models/tfmodel_normal/', stats=False, steps=10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d241a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow-2.5.0 (/usr/local/lib/python3.8/dist-packages/tensorflow/include, /usr/local/lib/python3.8/dist-packages/tensorflow)\n",
      "Namespace(batch=5000, dact='none-tanh-tanh', datadirs=['/tmp/tdata/'], ddrop=0.1, decay=0.1, dnet='27-34-2', ftest=0.25, load=False, lstep=100, mact='none-tanh-tanh', mdrop=0.1, mnet='27-34-2', mve=True, nosmax=False, output='models/tfmodel_normal', stats=False, steps=10000)\n",
      "8376 tuples have been loaded; randomly selected 6282 for the training set and 2094 for the test set\n",
      "{'inputs': 3.7656653, 'labels': 1.0, 'modvel': 2.749637}\n"
     ]
    }
   ],
   "source": [
    "pargs.output = os.path.normpath(pargs.output)\n",
    "os.path.isdir(pargs.output) or os.makedirs(pargs.output)\n",
    "\n",
    "shutil.copy(inspect.stack()[-1][1], pargs.output+'/')\n",
    "with open(pargs.output+'/run_args.pickle', 'wb') as f: \n",
    "    pickle.dump(vars(pargs), f)\n",
    "with open(pargs.output+'/run_cmd.txt', 'w') as f: \n",
    "    f.write(' '.join(os.uname()) + '\\n' + ' '.join(sys.argv))\n",
    "\n",
    "data_sets, N_tuple = tf_datasets.read_data_sets(dirs=sorted(pargs.datadirs), use_softmax=(not pargs.nosmax), frac_test=pargs.ftest)\n",
    "scale = { i: max(abs(data_sets.train.get_data()[i].min()), abs(data_sets.train.get_data()[i].max())) for i in data_sets.train.get_data() }\n",
    "with open(pargs.output+'/scale.pickle', 'wb') as f: \n",
    "    pickle.dump(scale, f)\n",
    "\n",
    "logging.basicConfig(filename='{}/training-info.log'.format(pargs.output), level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "logging.info('tensorflow-{} ({}, {})'.format(tf.__version__, tf.sysconfig.get_include(), tf.sysconfig.get_lib()))\n",
    "logging.info(pargs)\n",
    "logging.info('{} tuples have been loaded; randomly selected {} for the training set and {} for the test set'.format(\n",
    "    N_tuple, data_sets.train._num_examples, data_sets.test._num_examples))\n",
    "logging.info(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a1f927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24207886, -0.48731592],\n",
       "       [-0.09503105,  0.01840759],\n",
       "       [-0.00978813, -0.03523953],\n",
       "       ...,\n",
       "       [-0.03151188,  0.02256931],\n",
       "       [-0.19747956, -0.19343984],\n",
       "       [ 0.00856754,  0.0028379 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets.train.get_data()['modvel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa7c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "if pargs.stats and (not pargs.load):\n",
    "    with PdfPages(pargs.output+'/histogram.pdf') as pdf, open(pargs.output+'/dataset_stats.pickle', 'wb') as log:\n",
    "        l = data_sets.train.get_data()['labels']\n",
    "        if not pargs.nosmax: l = l[:,0]\n",
    "        dataset_stats = {}\n",
    "        for i in sorted(data_sets.train.get_data()):\n",
    "            d = data_sets.train.get_data()[i][(l==1).reshape(-1)] # only splash particles\n",
    "            dataset_stats[i] = [None]*d.shape[1]\n",
    "            for j in range(d.shape[1]):\n",
    "                d_row = d[:,j].reshape(-1)\n",
    "                dataset_stats[i][j] = { 'mean': np.mean(d_row), 'std': np.std(d_row), 'min': np.amin(d_row), 'max': np.amax(d_row) }\n",
    "                plt.figure()\n",
    "                plt.hist(d_row, bins='auto')\n",
    "                plt.title('Histogram of {}[{}]'.format(i, j))\n",
    "                plt.savefig(pdf, format='pdf')\n",
    "                plt.close()\n",
    "\n",
    "        pickle.dump(dataset_stats, log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163cf0",
   "metadata": {},
   "source": [
    "SAM is implemented as follows - \n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/qRSfNX7/image.png\"></img><br>\n",
    "<small>Source: Original Paper</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e28f01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neural network structure: detection 27-34-2 and modification 27-34-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 34, 2]\n",
      "[27, 34, 2]\n",
      "[27, 34, 2]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "################################################################################\n",
    "# neural networks\n",
    "logging.info('Neural network structure: detection {} and modification {}'.format(pargs.dnet, pargs.mnet))\n",
    "dlayers    = list(map(int, pargs.dnet.split('-')))\n",
    "mlayers    = list(map(int, pargs.mnet.split('-')))\n",
    "dact       = list(map(tf_network.parse_act, pargs.dact.split('-')))\n",
    "mact       = list(map(tf_network.parse_act, pargs.mact.split('-')))\n",
    "init_w     = {'w': {'stddev': 0.1}, 'b': {'value': 0.5}}\n",
    "x          = tf.placeholder(tf.float32, shape=[None, dlayers[0]], name='x-input')\n",
    "keep_prob  = tf.placeholder(tf.float32, name='keep_prob_detector') if pargs.ddrop>0.0 else None\n",
    "keep_prob2 = tf.placeholder(tf.float32, name='keep_prob_modifier') if pargs.mdrop>0.0 else None\n",
    "y_,  y     = tf_network.build_network(dlayers, dact, init_weights=init_w, input_x_holder=x, dropout_holder=keep_prob,  bn=True, scope='detector/')[1:]\n",
    "y2_, y2    = tf_network.build_network(mlayers, mact, init_weights=init_w, input_x_holder=x, dropout_holder=keep_prob2, bn=True, scope='modifier/')[1:]\n",
    "if pargs.mve:\n",
    "    s      = tf_network.build_network(mlayers, mact, init_weights=init_w, input_x_holder=x, input_y_holder=y2_, dropout_holder=keep_prob2, bn=True, scope='modifier_var/')[2]\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6eebb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 34, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d936823",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# evaluation functions\n",
    "log_dict = {}\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        if pargs.nosmax: corr, appx = tf.cast(tf.less(y_, 0.5), tf.int64), tf.cast(tf.less(y, 0.5), tf.int64) # f: splashing, t: non-splashing\n",
    "        else:            corr, appx = tf.argmax(y_, 1), tf.argmax(y, 1)                                       # 0: splashing, 1: non-splashing\n",
    "\n",
    "        correct_prediction = tf.equal(corr, appx)\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        y_b_appx           = tf.equal(appx, 0) # true: splashing, false: non-splashing\n",
    "        y_b_corr           = tf.equal(corr, 0)\n",
    "\n",
    "        N_corr_non_splas    = tf.reduce_sum(corr)\n",
    "        N_corr_splashing    = tf.cast(tf.shape(y_)[0], tf.int64) - N_corr_non_splas\n",
    "        diff_appx_corr      = tf.logical_xor(y_b_appx, y_b_corr)\n",
    "        N_corr_spl_appx_non = tf.reduce_sum(tf.cast(tf.logical_and(y_b_corr, diff_appx_corr), tf.float32))\n",
    "        N_corr_non_appx_spl = tf.reduce_sum(tf.cast(tf.logical_and(y_b_appx, diff_appx_corr), tf.float32))\n",
    "        false_negative      = N_corr_spl_appx_non/tf.cast(N_corr_splashing, tf.float32)\n",
    "        false_positive      = N_corr_non_appx_spl/tf.cast(N_corr_non_splas, tf.float32)\n",
    "\n",
    "        log_dict['accuracy']                     = accuracy\n",
    "        log_dict['false_negative_corr_T_appx_F'] = false_negative\n",
    "        log_dict['false_positive_corr_F_appx_T'] = false_positive\n",
    "        log_dict['splashes/corr']                = 1.0 - tf.reduce_mean(tf.cast(corr, tf.float32))\n",
    "        log_dict['splashes/appx']                = 1.0 - tf.reduce_mean(tf.cast(appx, tf.float32))\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss_normalizer = 1.0/tf.cast(tf.shape(y2_)[0], tf.float32)\n",
    "\n",
    "        with tf.name_scope('detector'):\n",
    "            if pargs.nosmax: loss_detector = tf.nn.l2_loss(y - y_)\n",
    "            else:            loss_detector = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "            log_dict['detector/loss'] = loss_detector*loss_normalizer if pargs.nosmax else loss_detector\n",
    "\n",
    "        with tf.name_scope('modifier'):\n",
    "            loss_modifier = tf.nn.l2_loss(y2 - y2_)\n",
    "            log_dict['modifier/loss'] = loss_modifier*loss_normalizer\n",
    "            if pargs.mve:\n",
    "                loss_modifier_mve = 0.5*tf.reduce_sum(((y2 - y2_)**2)/(s**2 + 1e-4)) + 0.5*tf.reduce_sum(tf.log(s**2 + 1e-4)) # mean variance estimate\n",
    "                log_dict['modifier_mve/loss'] = loss_modifier_mve*loss_normalizer\n",
    "\n",
    "        loss = loss_detector + loss_modifier\n",
    "        log_dict['sum_loss'] = log_dict['detector/loss'] + log_dict['modifier/loss']\n",
    "        if pargs.mve:\n",
    "            loss_mve = loss_detector + loss_modifier_mve\n",
    "            log_dict['sum_loss_mve'] = log_dict['detector/loss'] + log_dict['modifier_mve/loss']\n",
    "\n",
    "        if pargs.decay>0.0:\n",
    "            w_detector     = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"detector/\")\n",
    "            w_modifier     = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"modifier/\")\n",
    "            decay_detector = tf.add_n([tf.nn.l2_loss(v) for v in w_detector])*pargs.decay\n",
    "            decay_modifier = tf.add_n([tf.nn.l2_loss(v) for v in w_modifier])*pargs.decay\n",
    "\n",
    "            loss += decay_detector + decay_modifier\n",
    "\n",
    "            log_dict['detector/decay'] = decay_detector\n",
    "            log_dict['modifier/decay'] = decay_modifier\n",
    "            log_dict['sum_loss'] += decay_detector + decay_modifier\n",
    "\n",
    "            if pargs.mve:\n",
    "                w_modifier_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"modifier_var/\")\n",
    "                decay_modifier_var = tf.add_n([tf.nn.l2_loss(v) for v in w_modifier_var])*pargs.decay\n",
    "                loss_mve += decay_modifier_var\n",
    "                log_dict['modifier_var/decay'] = decay_modifier_var\n",
    "                log_dict['sum_loss_mve'] += decay_modifier_var\n",
    "\n",
    "    for i in log_dict:\n",
    "        tf.summary.scalar(name=i, tensor=log_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3ec573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'modifier_var/layer1/weights/Variable:0' shape=(27, 34) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer1/biases/Variable:0' shape=(34,) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer1/BN_Wx_plus_b/Variable:0' shape=(34,) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer1/BN_Wx_plus_b/Variable_1:0' shape=(34,) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer_full/weights/Variable:0' shape=(34, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer_full/biases/Variable:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer_full/BN_Wx_plus_b/Variable:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'modifier_var/layer_full/BN_Wx_plus_b/Variable_1:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_modifier_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320df998",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    if pargs.mve: train_step_mve = tf.train.AdamOptimizer(1e-4).minimize(loss_mve)\n",
    "\n",
    "increment_global_step = tf.assign_add(global_step, 1, name='increment_global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a05a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "detector/layer1/weights/Variable:0 (float32_ref 27x34) [918, bytes: 3672]\n",
      "detector/layer1/biases/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "detector/layer1/BN_Wx_plus_b/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "detector/layer1/BN_Wx_plus_b/Variable_1:0 (float32_ref 34) [34, bytes: 136]\n",
      "detector/layer_full/weights/Variable:0 (float32_ref 34x2) [68, bytes: 272]\n",
      "detector/layer_full/biases/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "detector/layer_full/BN_Wx_plus_b/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "detector/layer_full/BN_Wx_plus_b/Variable_1:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier/layer1/weights/Variable:0 (float32_ref 27x34) [918, bytes: 3672]\n",
      "modifier/layer1/biases/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier/layer1/BN_Wx_plus_b/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier/layer1/BN_Wx_plus_b/Variable_1:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier/layer_full/weights/Variable:0 (float32_ref 34x2) [68, bytes: 272]\n",
      "modifier/layer_full/biases/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier/layer_full/BN_Wx_plus_b/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier/layer_full/BN_Wx_plus_b/Variable_1:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier_var/layer1/weights/Variable:0 (float32_ref 27x34) [918, bytes: 3672]\n",
      "modifier_var/layer1/biases/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier_var/layer1/BN_Wx_plus_b/Variable:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier_var/layer1/BN_Wx_plus_b/Variable_1:0 (float32_ref 34) [34, bytes: 136]\n",
      "modifier_var/layer_full/weights/Variable:0 (float32_ref 34x2) [68, bytes: 272]\n",
      "modifier_var/layer_full/biases/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier_var/layer_full/BN_Wx_plus_b/Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "modifier_var/layer_full/BN_Wx_plus_b/Variable_1:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 3282\n",
      "Total bytes of variables: 13128\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.contrib.slim as slim\n",
    "import tf_slim as slim\n",
    "\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0: accuracy=0.52722, detector/loss=0.76243, modifier/loss=0.41393, modifier_mve/loss=40.22669, sum_loss=6.40668, sum_loss_mve=43.62448\n",
      "At step 100: accuracy=0.69962, detector/loss=0.56930, modifier/loss=0.33604, modifier_mve/loss=30.48972, sum_loss=6.02030, sum_loss_mve=33.69437\n",
      "At step 200: accuracy=0.75597, detector/loss=0.51873, modifier/loss=0.29188, modifier_mve/loss=27.33919, sum_loss=5.81308, sum_loss_mve=30.49327\n",
      "At step 300: accuracy=0.76886, detector/loss=0.50158, modifier/loss=0.26017, modifier_mve/loss=25.17930, sum_loss=5.65577, sum_loss_mve=28.31623\n",
      "At step 400: accuracy=0.77746, detector/loss=0.49434, modifier/loss=0.24887, modifier_mve/loss=24.00844, sum_loss=5.53265, sum_loss_mve=27.13813\n",
      "At step 500: accuracy=0.78080, detector/loss=0.48958, modifier/loss=0.24200, modifier_mve/loss=23.52537, sum_loss=5.42048, sum_loss_mve=26.65031\n",
      "At step 600: accuracy=0.78319, detector/loss=0.48597, modifier/loss=0.23584, modifier_mve/loss=23.28539, sum_loss=5.31468, sum_loss_mve=26.40671\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# load / train\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if pargs.load:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(pargs.output))\n",
    "    last_step = sess.run(global_step)\n",
    "    logging.info('Trained model is loaded ({})'.format(pargs.output))\n",
    "\n",
    "else:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(pargs.output + '/summary/train')\n",
    "test_writer  = tf.summary.FileWriter(pargs.output + '/summary/test')\n",
    "if not pargs.load: train_writer.add_graph(sess.graph)\n",
    "\n",
    "batch_test = data_sets.test.get_data()\n",
    "feed_data_test = { x: batch_test['inputs'], y_: batch_test['labels'], y2_: batch_test['modvel']/scale['modvel'] }\n",
    "if pargs.ddrop>0.0: feed_data_test.update({keep_prob : 1.0})\n",
    "if pargs.mdrop>0.0: feed_data_test.update({keep_prob2: 1.0})\n",
    "\n",
    "losses_key = sorted([k for k in log_dict if 'loss' in k])\n",
    "losses_fetch = [log_dict[k] for k in losses_key]\n",
    "for i in range(last_step if pargs.load else 0, pargs.steps):\n",
    "    batch = data_sets.train.next_batch(pargs.batch)\n",
    "\n",
    "    fetches = [merged, train_step, increment_global_step]\n",
    "    feed_data = { x: batch['inputs'], y_: batch['labels'], y2_: batch['modvel']/scale['modvel'] }\n",
    "\n",
    "    if pargs.mve and i>int(pargs.steps/2):\n",
    "        fetches = [merged, train_step_mve, increment_global_step]\n",
    "        if pargs.mdrop>0.0: y2_appx = sess.run(y2, feed_dict={ x: batch['inputs'], keep_prob2: 1.0 })\n",
    "        else:               y2_appx = sess.run(y2, feed_dict={ x: batch['inputs'] })\n",
    "        feed_data = { x: batch['inputs'], y_: batch['labels'], y2_: batch['modvel']/scale['modvel'], y2: y2_appx }\n",
    "\n",
    "    if pargs.ddrop>0.0: feed_data.update({keep_prob : 1.0-pargs.ddrop})\n",
    "    if pargs.mdrop>0.0: feed_data.update({keep_prob2: 1.0-pargs.mdrop})\n",
    "\n",
    "    params_sess_run = dict(fetches=fetches, feed_dict=feed_data)\n",
    "    if (i % pargs.lstep*10 == 0):\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        params_sess_run.update(options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\n",
    "\n",
    "    summary = sess.run(**params_sess_run)[0]\n",
    "\n",
    "    if (i % pargs.lstep*10 == 0):\n",
    "        train_writer.add_run_metadata(run_metadata, 'step{:03d}'.format(i))\n",
    "\n",
    "    if (i % pargs.lstep == 0):           # evaluate losses with the test data set\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "    if (i % pargs.lstep == 0):           # evaluate losses with the test data set\n",
    "        summary, acc, losses = sess.run([merged, accuracy, losses_fetch], feed_dict=feed_data_test)\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('At step {}: accuracy={:.5f}, {}'.format(i, acc, ', '.join('{}={:.5f}'.format(*t) for t in zip(losses_key, losses))))\n",
    "\n",
    "# the last test\n",
    "summary, acc_summary = sess.run([merged, list(log_dict.values())], feed_dict=feed_data_test)\n",
    "test_writer.add_summary(summary, i)\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "logging.info(dict(zip(log_dict.keys(), acc_summary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95344501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model_file = saver.save(sess, '{}/model.ckpt'.format(pargs.output))\n",
    "logging.info('Trained model saved to {}'.format(model_file))\n",
    "\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_sess = tf.InteractiveSession()\n",
    "\n",
    "# tfopt = pickle.load(open(pargs.output + '/run_args.pickle', 'rb'))\n",
    "\n",
    "# dlayers = list(map(int, tfopt['dnet'].split('-')))\n",
    "# mlayers = list(map(int, tfopt['mnet'].split('-')))\n",
    "# dact    = list(map(tf_network.parse_act, tfopt['dact'].split('-')))\n",
    "# mact    = list(map(tf_network.parse_act, tfopt['mact'].split('-')))\n",
    "# x       = tf.placeholder(tf.float32, shape=[None, dlayers[0]], name='x-input')\n",
    "# y_,  y  = tf_network.build_network(dlayers, dact, input_x_holder=x, bn=True, is_training=False, scope='detector/')[1:]\n",
    "# y2_, y2 = tf_network.build_network(mlayers, mact, input_x_holder=x, bn=True, is_training=False, scope='modifier/')[1:]\n",
    "# if tfopt['mve']:\n",
    "#     sd  = tf_network.build_network(mlayers, mact, input_x_holder=x, input_y_holder=y2_, bn=True, is_training=False, scope='modifier_var/')[2]\n",
    "\n",
    "\n",
    "# tf_saver = tf.train.Saver()\n",
    "# tf_saver.restore(tf_sess, pargs.output + '/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1976ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_sess.run([y, y2, s], feed_dict={x: data_sets.test.get_data()[\"inputs\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
